{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54472e8",
   "metadata": {},
   "source": [
    "# Classification benchmark: PLN-Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6167b8ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:10.990460Z",
     "start_time": "2024-06-11T14:25:07.844263Z"
    },
    "scrolled": true
   },
   "source": [
    "from plntree.models import PLNTreeConditional, PLNTree\n",
    "from plntree.utils.classifiers import DenseClassifier, RNNClassifier\n",
    "from plntree.utils.jupyter_functions import *\n",
    "from plntree.data.utils import numpy_dataset_to_torch_dataloader\n",
    "import torch\n",
    "import torch.optim as optim"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ed196583",
   "metadata": {},
   "source": [
    "# Metagenomics dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4965de45",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddab4599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:10.993953Z",
     "start_time": "2024-06-11T14:25:10.991437Z"
    }
   },
   "source": [
    "seed = 0\n",
    "# Select the taxonomic precision\n",
    "precision = 'f'\n",
    "prefix = f'metagenomics_classification_taxaL{precision}_s{seed}'\n",
    "\n",
    "seed_all(seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc3720e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:11.751757Z",
     "start_time": "2024-06-11T14:25:10.995111Z"
    }
   },
   "source": [
    "from plntree.data import metagenomics_loader\n",
    "\n",
    "raw_abundances, metadata = metagenomics_loader.load_raw_data(directory='./plntree/data/metagenomics')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37cc845d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:11.768445Z",
     "start_time": "2024-06-11T14:25:11.752705Z"
    }
   },
   "source": [
    "diseases = ['t2d', 'ibd_ulcerative_colitis', 'cirrhosis', 'ibd_crohn_disease', 'cancer', 'obesity', 'leaness']\n",
    "raw_abundances, metadata = metagenomics_loader.filter_diseases(raw_abundances, metadata, diseases)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf886ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:11.771636Z",
     "start_time": "2024-06-11T14:25:11.769331Z"
    }
   },
   "source": [
    "# Filter out some taxa\n",
    "filtered_taxa = ['k__Archaea', 'k__Eukaryota']\n",
    "# Select the rarefaction offset\n",
    "rarefaction_offset = 12\n",
    "# Select the prevalence threshold (-1 for none)\n",
    "prevalence = 1 / np.exp(rarefaction_offset)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcad9e04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:12.204574Z",
     "start_time": "2024-06-11T14:25:11.772724Z"
    }
   },
   "source": [
    "abundances = metagenomics_loader.raw_abundance_filter(raw_abundances, precision)\n",
    "for taxon in filtered_taxa:\n",
    "    abundances = metagenomics_loader.exclude_taxon(abundances, taxon)\n",
    "if prevalence > 0:\n",
    "    abundances = metagenomics_loader.prevalence_filter(abundances, threshold=prevalence)\n",
    "abundances = metagenomics_loader.rarefy(abundances, rarefaction_offset, seed=seed)\n",
    "\n",
    "taxonomy = metagenomics_loader.get_taxonomy(abundances)\n",
    "taxonomy.plot(legend=False, title='')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4c9386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:12.531767Z",
     "start_time": "2024-06-11T14:25:12.205528Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Select the offset layer\n",
    "selected_layers = [2, -1]\n",
    "K = list(taxonomy.getLayersWidth().values())[selected_layers[0]:]\n",
    "X_base, patient_ids = metagenomics_loader.hierarchical_dataset(abundances, taxonomy, offset_layer=selected_layers[0])\n",
    "\n",
    "# Select the batch size\n",
    "batch_size = len(X_base)\n",
    "seed_all(seed)\n",
    "dataset = TensorDataset(X_base.to(dtype=torch.float64), X_base.to(dtype=torch.float64))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cee2d33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:13.179450Z",
     "start_time": "2024-06-11T14:25:12.532515Z"
    }
   },
   "source": [
    "vizualize_samples(dataloader, taxonomy, selected_layers, autofill=True, seed=seed)\n",
    "savefig('metagenomics_samples')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b6ac34c9",
   "metadata": {},
   "source": [
    "## Visualize the problem's difficulty with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f8000b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:15.351550Z",
     "start_time": "2024-06-11T14:25:13.180170Z"
    }
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib\n",
    "\n",
    "Y_base = metadata['disease']\n",
    "def plot_pca(Y_base, X_base):\n",
    "    fig, axs = plt.subplots(1, len(K), figsize=(15, 4))\n",
    "    colors = [matplotlib.cm.get_cmap('viridis')((i+1)/len(np.unique(Y_base))) for i, name in enumerate(np.unique(Y_base))]\n",
    "    for layer, K_l in enumerate(K):\n",
    "        X_l = X_base[:, layer, :K_l]\n",
    "        pca = PCA(n_components=2, random_state=seed).fit(X_l)\n",
    "        X_l_pca = pca.transform(X_l)\n",
    "        for k, c in enumerate(np.unique(Y_base)):\n",
    "            indexes = np.where(Y_base == c)\n",
    "            axs[layer].set_title(f'$\\ell = {layer + selected_layers[0]}$')\n",
    "            axs[layer].plot(X_l_pca[indexes, 0], X_l_pca[indexes, 1], marker='.', linestyle='', color=colors[k], alpha=0.5)\n",
    "            axs[layer].set_xlabel(f'axis 1 ({np.round(pca.explained_variance_ratio_[0] * 100, 1)}%)')\n",
    "            axs[layer].set_ylabel(f'axis 2 ({np.round(pca.explained_variance_ratio_[1] * 100, 1)}%)')\n",
    "    legend_handles = [\n",
    "            mlines.Line2D([], [], marker='o', linestyle='', color=color, alpha=0.9, label=group)\n",
    "            for color, group in zip(colors, np.unique(Y_base))\n",
    "        ]\n",
    "    legend = plt.legend(handles=legend_handles, fontsize=\"12\", loc='lower center', bbox_to_anchor=(-1.65, -0.32), ncols=len(colors))\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "plot_pca(Y_base, X_base / X_base[:, 0].sum(dim=-1, keepdims=True).unsqueeze(-1))\n",
    "savefig(f'{prefix}_PCA_all_diseases')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cee2d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:15.356240Z",
     "start_time": "2024-06-11T14:25:15.353238Z"
    }
   },
   "source": [
    "n_classes = len(np.unique(Y_base))\n",
    "n_classes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c96f08e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:15.361307Z",
     "start_time": "2024-06-11T14:25:15.357956Z"
    }
   },
   "source": [
    "print(np.unique(Y_base, return_counts=True))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "068bb76e",
   "metadata": {},
   "source": [
    "## Preprocessing using PLN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6310359e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:25.838004Z",
     "start_time": "2024-06-11T14:25:15.362576Z"
    },
    "scrolled": true
   },
   "source": [
    "pln_layers = learn_pln(X_base, K, seed=seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6432ab57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:25.853188Z",
     "start_time": "2024-06-11T14:25:25.838728Z"
    },
    "scrolled": true
   },
   "source": [
    "def get_plntree_params_and_suffix(n_latent_layers, variational_approx, variational_approx_params):\n",
    "    params = {\n",
    "        'tree':taxonomy,\n",
    "        'selected_layers':selected_layers,\n",
    "        'diagonal_model':False,\n",
    "        'variational_approx':variational_approx,\n",
    "        'positive_fun':'softplus',\n",
    "        'offset_method':'constant',\n",
    "        'variational_approx_params':variational_approx_params,\n",
    "        'n_latent_layers':n_latent_layers,\n",
    "        'diag_smoothing_factor':1e-4\n",
    "    }\n",
    "    placeholder = variational_approx_params\n",
    "    if variational_approx == 'mean_field':\n",
    "        placeholder = variational_approx_params['n_variational_layers']\n",
    "    suffix = f'latentlayers-{n_latent_layers}_varlayers-{placeholder}'\n",
    "    return params, suffix\n",
    "\n",
    "def learn_plntree_mean_field(params):\n",
    "    estimator = PLNTree(**params, seed=seed)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        estimator.parameters(),\n",
    "        lr=1e-3,\n",
    "    )\n",
    "    n_epoch = 30_000\n",
    "    return estimator.fit(optimizer, dataloader, n_epoch=n_epoch, verbose=100, max_grad_norm=10.)\n",
    "\n",
    "n_latent_layers = 1\n",
    "n_variational_layers = 3\n",
    "preprocessing = 'proportion'\n",
    "print('Latents size', n_latent_layers)\n",
    "print('Variational size', n_variational_layers)\n",
    "variational_approx_params = {\n",
    "    'n_variational_layers':n_variational_layers,\n",
    "    'preprocessing': [preprocessing]\n",
    "}\n",
    "params, suffix = get_plntree_params_and_suffix(n_latent_layers, 'mean_field', variational_approx_params)\n",
    "try:\n",
    "    meanfield, meanfield_losses = load_pkl(prefix, f'mean_field_{preprocessing}_{suffix}')\n",
    "except:\n",
    "    meanfield, meanfield_losses = learn_plntree_mean_field(params)\n",
    "    save_pkl((meanfield, meanfield_losses), prefix, f'mean_field_{preprocessing}_{suffix}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5744a0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:26.012381Z",
     "start_time": "2024-06-11T14:25:25.854733Z"
    }
   },
   "source": [
    "fig, axs = plt.subplots()\n",
    "axs.plot(meanfield_losses)\n",
    "axs.set_yscale('log')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2065ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:26.025024Z",
     "start_time": "2024-06-11T14:25:26.013209Z"
    }
   },
   "source": [
    "preprocessing = 'proportion'\n",
    "def learn_plntree(n_latent_layers, variational_approx, variational_approx_params):\n",
    "    estimator = PLNTree(\n",
    "        tree=taxonomy,\n",
    "        selected_layers=selected_layers,\n",
    "        diag_smoothing_factor=1e-4,\n",
    "        positive_fun='softplus',\n",
    "        offset_method='constant',\n",
    "        identifiable=True,\n",
    "        variational_approx=variational_approx,\n",
    "        variational_approx_params=variational_approx_params,\n",
    "        n_latent_layers=n_latent_layers,\n",
    "        seed=seed\n",
    "    )\n",
    "    optimizer = optim.Adam(\n",
    "        estimator.parameters(),\n",
    "        lr=1e-3,\n",
    "    )\n",
    "    estimator.to(dtype=torch.float64)\n",
    "    n_epoch = 30_000\n",
    "    return estimator.fit(optimizer, dataloader, n_epoch=n_epoch, verbose=100, max_grad_norm=5.)\n",
    "\n",
    "def embedder_params(embedder_type='GRU', embedding_size=16, n_embedding_layers=2, n_embedding_neurons=32, n_after_layers=2):\n",
    "    params = {\n",
    "        'embedder_type': embedder_type,\n",
    "        'embedding_size': embedding_size,\n",
    "        'n_embedding_layers': n_embedding_layers,\n",
    "        'n_embedding_neurons': n_embedding_neurons,\n",
    "        'n_after_layers': n_after_layers,\n",
    "        'preprocessing': ['proportion']\n",
    "    }\n",
    "    name = f'Emb{embedder_type}-{n_embedding_layers}x{n_embedding_neurons}to{embedding_size}-{n_after_layers}'\n",
    "    return name, params\n",
    "\n",
    "n_latent_layers = 1\n",
    "emb_name, variational_approx_params = embedder_params(\n",
    "    embedding_size=32,\n",
    "    n_embedding_layers=2,\n",
    "    n_embedding_neurons=64,\n",
    "    n_after_layers=2\n",
    ")\n",
    "try:\n",
    "    backward, backward_losses = load_pkl(prefix, f'residual_backward_{preprocessing}_{n_latent_layers}-{emb_name}')\n",
    "except:\n",
    "    print('Learning PLN-Tree (residual backward)')\n",
    "    print(variational_approx_params)\n",
    "    backward, backward_losses = learn_plntree(n_latent_layers, 'residual_backward', variational_approx_params)\n",
    "    save_pkl((backward, backward_losses), prefix, f'residual_backward_{preprocessing}_{n_latent_layers}-{emb_name}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad818c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:26.259330Z",
     "start_time": "2024-06-11T14:25:26.026225Z"
    }
   },
   "source": [
    "fig, axs = plt.subplots()\n",
    "axs.plot(backward_losses)\n",
    "axs.set_yscale('log')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "937b7b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:26.479779Z",
     "start_time": "2024-06-11T14:25:26.260394Z"
    }
   },
   "source": [
    "Z_meanfield, _ = meanfield.encode(X_base, seed=42)\n",
    "Z_backward, _ = backward.encode(X_base, seed=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4425046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:36.429268Z",
     "start_time": "2024-06-11T14:25:26.480830Z"
    }
   },
   "source": [
    "Z_pln_enc = generate_pln_data(pln_layers, 1, K, selected_layers, X_base, taxonomy, seed=seed)[-2]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be449a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:37.751133Z",
     "start_time": "2024-06-11T14:25:36.430132Z"
    }
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib\n",
    "\n",
    "Y_base = metadata['disease'].copy()\n",
    "prefix_labels = 'all_diseases-'\n",
    "plot_pca(Y_base, Z_backward.detach())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73cffc11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:25:39.925196Z",
     "start_time": "2024-06-11T14:25:37.752298Z"
    }
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib\n",
    "\n",
    "Y_base = metadata['disease'].copy()\n",
    "prefix_labels = 'all_diseases-'\n",
    "plot_pca(Y_base, torch.log(backward.latent_tree_allocation(Z_backward)).detach())\n",
    "savefig(f'{prefix}_PCA_all_diseases_latent_allocation')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a8ea0e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:07.381908Z",
     "start_time": "2024-06-11T14:27:06.138903Z"
    }
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib\n",
    "\n",
    "scenario = 't2d'\n",
    "\n",
    "if scenario == 'ibd':\n",
    "    filtered_labels = ['ibd_crohn_disease', 'ibd_ulcerative_colitis']\n",
    "elif scenario == 'cirrhosis':\n",
    "    filtered_labels = ['cirrhosis']\n",
    "elif scenario == 'obesity':\n",
    "    filtered_labels = ['obesity']\n",
    "elif scenario == 'colorectal':\n",
    "    filtered_labels = ['cancer']\n",
    "elif scenario == 'leaness':\n",
    "    filtered_labels = ['leaness']\n",
    "elif scenario == 't2d':\n",
    "    filtered_labels = ['t2d']\n",
    "elif scenario == 't2d_obe':\n",
    "    filtered_labels = ['t2d', 'obesity']\n",
    "elif scenario == 'cirr_col':\n",
    "    filtered_labels = ['cirrhosis', 'colorectal']\n",
    "prefix_labels = ''\n",
    "for label in filtered_labels:\n",
    "    prefix_labels += f'-{label}'\n",
    "Y_base = metadata['disease'].copy()\n",
    "for i in range(len(Y_base)):\n",
    "    if Y_base[i] not in filtered_labels:\n",
    "        Y_base[i] = 'Others'\n",
    "    else:\n",
    "        Y_base[i] = 'Targets'\n",
    "\n",
    "plot_pca(Y_base, torch.log(backward.latent_tree_allocation(Z_backward)).detach())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6454d6f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:07.385703Z",
     "start_time": "2024-06-11T14:27:07.382770Z"
    }
   },
   "source": [
    "n_classes = len(np.unique(Y_base))\n",
    "n_classes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a16a4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:07.391050Z",
     "start_time": "2024-06-11T14:27:07.387025Z"
    }
   },
   "source": [
    "print(np.unique(Y_base, return_counts=True))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "967be76f",
   "metadata": {},
   "source": [
    "## Benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b010417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:07.397080Z",
     "start_time": "2024-06-11T14:27:07.392675Z"
    }
   },
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def train_test(X, y, train_indexes, test_indexes):\n",
    "    X_train, y_train = X[train_indexes], y[train_indexes]\n",
    "    X_test, y_test = X[test_indexes], y[test_indexes]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def bootstrap_train(X, y, model, params, n_repeats=10, train_size=0.8, seed=None, verbose=False):\n",
    "    seed_all(seed)\n",
    "    models = []\n",
    "    indexes = np.arange(0, len(X))\n",
    "    for i, (train_indexes, test_indexes) in enumerate(StratifiedShuffleSplit(n_splits=n_repeats, train_size=train_size).split(X, y)):\n",
    "        X_train, y_train, X_test, y_test = train_test(X, y, train_indexes, test_indexes)\n",
    "        assert((np.unique(y_train) == np.unique(y_test)).all())\n",
    "        if verbose:\n",
    "            print(f'Fitting Fold {i}...')\n",
    "        benchmark = model(**params).fit(X_train, y_train)\n",
    "        models.append((benchmark, train_indexes, test_indexes))\n",
    "    return models"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "215c8449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:07.411112Z",
     "start_time": "2024-06-11T14:27:07.398292Z"
    },
    "code_folding": [
     43
    ]
   },
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, average_precision_score, RocCurveDisplay\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "from scipy.stats import mstats\n",
    "import math\n",
    "\n",
    "def performances(X, y, benchmarks, predict_args=None):\n",
    "    if predict_args is None:\n",
    "        predict_args = {}\n",
    "    metrics = {\n",
    "        'accuracy':accuracy_score,\n",
    "        'precision':lambda y_test, y_pred: precision_score(y_test, y_pred, average='weighted'),\n",
    "        'recall':lambda y_test, y_pred: recall_score(y_test, y_pred, average='weighted'),\n",
    "        'F1':lambda y_test, y_pred: f1_score(y_test, y_pred, average='weighted'),\n",
    "        'balanced accuracy':balanced_accuracy_score,\n",
    "    }\n",
    "    results = {f'{key}':[] for key in metrics}\n",
    "    conf_matrices = []\n",
    "    auc_args = []\n",
    "    pr_args = []\n",
    "    auc_values = []\n",
    "    auc_pr_values = []\n",
    "    pr_values = []\n",
    "    for model, train_indexes, test_indexes in benchmarks:\n",
    "        X_train, y_train, X_test, y_test = train_test(X, y, train_indexes, test_indexes)\n",
    "        y_proba = model.predict_proba(X_test, **predict_args)\n",
    "        if len(predict_args) == 0:\n",
    "            y_pred = model.predict(X_test)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test, predict_args)\n",
    "        for metric, metric_fun in metrics.items():\n",
    "            results[metric].append(metric_fun(y_test, y_pred))\n",
    "            \n",
    "        conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        if len(np.unique(y)) == 2:\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_proba[:, 1])\n",
    "            base_fpr = np.linspace(0, 1, 101)\n",
    "            tpr_interp = interp(base_fpr, fpr, tpr)\n",
    "            tpr_interp[0] = 0.0\n",
    "            auc_args.append(tpr_interp)\n",
    "            auc_values.append(auc(fpr, tpr))\n",
    "            \n",
    "            precisions, recalls, pr_thresh = precision_recall_curve(y_test, y_proba[:, 1])\n",
    "            base_recalls = np.linspace(0, 1, 101)\n",
    "            pre_interp = interp(base_recalls, np.flip(recalls), np.flip(precisions))\n",
    "            pre_interp[0] = np.flip(precisions)[0]\n",
    "            pr_args.append(pre_interp)\n",
    "            auc_pr_values.append(auc(recalls, precisions))\n",
    "            pr_values.append(average_precision_score(y_test, y_proba[:, 1]))\n",
    "    if len(auc_values) > 0:\n",
    "        results['ROC AUC'] = auc_values\n",
    "    if len(pr_values) > 0:\n",
    "        results['PR AUC'] = auc_pr_values\n",
    "        results['Averaged Precision'] = pr_values\n",
    "    return results, conf_matrices, auc_args, pr_args\n",
    "\n",
    "def performances_summary(X, y, benchmarks, name, predict_args=None, percentile=0.05):\n",
    "    np.set_printoptions(suppress=True)\n",
    "    results, conf_matrices, auc_args = performances(X, y, benchmarks, predict_args)\n",
    "    print(f'------- Model summary {name} -------')\n",
    "    for metric, res in results.items():\n",
    "        print(f'{metric}:', np.round(np.mean(res), decimals=3), f'({np.round(np.std(res), decimals=3)})')\n",
    "    cm = 0\n",
    "    for matrix in conf_matrices:\n",
    "        cm += matrix\n",
    "    print('Mean normalized confusion matrix:')\n",
    "    cm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "    labels = np.unique(y)\n",
    "    if len(auc_args) > 0:\n",
    "        fig, axs = plt.subplots()\n",
    "        base_fpr = np.linspace(0, 1, 101)\n",
    "        axs.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "        axs.set_xlim([-0.01, 1.0])\n",
    "        axs.set_ylim([-0.01, 1.05])\n",
    "        axs.set_ylabel('True positive rate', fontsize=12)\n",
    "        axs.set_xlabel('False positive rate', fontsize=12)\n",
    "        axs.tick_params(labelright=True)\n",
    "        axs.grid('True')\n",
    "\n",
    "        roc_tpr_array = np.array(auc_args)\n",
    "        for i in range(len(roc_tpr_array)):\n",
    "            for j in range(roc_tpr_array.shape[1]):\n",
    "                if math.isnan(roc_tpr_array[i][j]):\n",
    "                    roc_tpr_array[i][j] = 0\n",
    "        mean_tprs = roc_tpr_array.mean(axis=0)\n",
    "        std_tprs = roc_tpr_array.std(axis=0)\n",
    "        axs.plot(base_fpr, mean_tprs, 'b', label=\"Mean\")\n",
    "        \n",
    "        quantiles = mstats.mquantiles(roc_tpr_array, prob=[percentile, 1-percentile], axis=0)\n",
    "        axs.fill_between(base_fpr, quantiles[0], quantiles[1], color='b', alpha=0.1, label=f\"CI {int(100*(1-percentile))}%\")\n",
    "        axs.legend()\n",
    "    return pd.DataFrame(index=labels, columns=labels, data=cm).round(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3149499c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:07.418094Z",
     "start_time": "2024-06-11T14:27:07.411939Z"
    }
   },
   "source": [
    "import matplotlib\n",
    "def multiple_performances_summary(X_dic, y, model, params, n_repeats=50, train_size=0.8, percentile=0.05, save_name=None, seed=None, verbose=False):\n",
    "    df = None\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axs[0].plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "    axs[1].plot([0, 1], [0.5, 0.5], color=\"navy\", linestyle=\"--\")\n",
    "    colors = [matplotlib.cm.get_cmap('plasma')(1e-8 + (i+1)/(len(X_dic)+.5)) for i in range(len(X_dic))]\n",
    "    for k, (name, X) in enumerate(X_dic.items()):\n",
    "        benchmarks = bootstrap_train(\n",
    "            X, y,\n",
    "            model, params,\n",
    "            seed=seed,\n",
    "            n_repeats=n_repeats\n",
    "        )\n",
    "        np.set_printoptions(suppress=True)\n",
    "        results, conf_matrices, auc_args, pre_args = performances(X, y, benchmarks)\n",
    "        inputs = []\n",
    "        metrics = []\n",
    "        for metric, res in results.items():\n",
    "            inputs.append(f'{np.round(np.mean(res), decimals=3)} ({np.round(np.std(res), decimals=3)})')\n",
    "            metrics.append(metric)\n",
    "        if df is None:\n",
    "            df = pd.DataFrame(columns=metrics)\n",
    "        df.loc[name] = inputs\n",
    "        if len(auc_args) > 0:\n",
    "            base_fpr = np.linspace(0, 1, 101)\n",
    "            axs[0].set_xlim([-0.01, 1.0])\n",
    "            axs[0].set_ylim([-0.01, 1.05])\n",
    "            axs[0].set_ylabel('True positive rate', fontsize=12)\n",
    "            axs[0].set_xlabel('False positive rate', fontsize=12)\n",
    "\n",
    "            roc_tpr_array = np.array(auc_args)\n",
    "            for i in range(len(roc_tpr_array)):\n",
    "                for j in range(roc_tpr_array.shape[1]):\n",
    "                    if math.isnan(roc_tpr_array[i][j]):\n",
    "                        roc_tpr_array[i][j] = 0\n",
    "            mean_tprs = roc_tpr_array.mean(axis=0)\n",
    "            std_tprs = roc_tpr_array.std(axis=0)\n",
    "            axs[0].plot(base_fpr, mean_tprs, color=colors[k], label=name, alpha=0.7)\n",
    "\n",
    "            quantiles = mstats.mquantiles(roc_tpr_array, prob=[percentile, 1-percentile], axis=0)\n",
    "            axs[0].fill_between(base_fpr, quantiles[0], quantiles[1], color=colors[k], alpha=0.1)\n",
    "        if len(pre_args) > 0:\n",
    "            base_recalls = np.linspace(0, 1, 101)\n",
    "            axs[1].set_xlim([-0.01, 1.0])\n",
    "            axs[1].set_ylim([-0.01, 1.05])\n",
    "            axs[1].set_ylabel('Precision', fontsize=12)\n",
    "            axs[1].set_xlabel('Recall', fontsize=12)\n",
    "\n",
    "            precisions = np.array(pre_args)\n",
    "            for i in range(len(precisions)):\n",
    "                for j in range(precisions.shape[1]):\n",
    "                    if math.isnan(precisions[i][j]):\n",
    "                        precisions[i][j] = 0.\n",
    "            mean_precisions = precisions.mean(axis=0)\n",
    "            std_precisions = precisions.std(axis=0)\n",
    "            axs[1].plot(base_recalls, mean_precisions, color=colors[k], label=name, alpha=0.7)\n",
    "\n",
    "            quantiles = mstats.mquantiles(precisions, prob=[percentile, 1-percentile], axis=0)\n",
    "            axs[1].fill_between(base_recalls, quantiles[0], quantiles[1], color=colors[k], alpha=0.1)\n",
    "    #axs[0].legend()\n",
    "    axs[0].grid('True')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid('True')\n",
    "    plt.tight_layout()\n",
    "    if save_name is not None:\n",
    "        savefig(f'{prefix}_{save_name}')\n",
    "    return df.transpose()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f21f980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:07.630212Z",
     "start_time": "2024-06-11T14:27:07.624165Z"
    }
   },
   "source": [
    "# Raw data\n",
    "X = X_base[:, -1, :] / X_base[:, 0].sum(dim=-1, keepdims=True)\n",
    "y = Y_base.copy()\n",
    "y[y == 'Targets'] = 1\n",
    "y[y == 'Others'] = 0\n",
    "y = y.astype(np.int32)\n",
    "#y = y.factorize()[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c9165ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:07.996473Z",
     "start_time": "2024-06-11T14:27:07.983459Z"
    }
   },
   "source": [
    "\n",
    "# Preprocessed Z\n",
    "Z_meanfield_alloc = meanfield.latent_tree_allocation(Z_meanfield)[:, -1, :].detach()\n",
    "Z_backward_alloc = backward.latent_tree_allocation(Z_backward)[:, -1, :].detach()\n",
    "\n",
    "Z_meanfield_alloc = torch.log(Z_meanfield_alloc)\n",
    "Z_backward_alloc = torch.log(Z_backward_alloc)\n",
    "\n",
    "Z_pln = Z_pln_enc[:, -1, :]\n",
    "\n",
    "from sklearn import preprocessing\n",
    "#X = preprocessing.MaxAbsScaler().fit_transform(X)\n",
    "#Z_meanfield_alloc = preprocessing.StandardScaler().fit_transform(Z_meanfield_alloc)\n",
    "#Z_backward_alloc = preprocessing.MinMaxScaler().fit_transform(Z_backward_alloc)\n",
    "\n",
    "#Z_backward_alloc = Z_backward[:, -1, :].detach()\n",
    "\n",
    "X_dic = {'raw':X, 'backward':Z_backward_alloc, 'MF':Z_meanfield_alloc, 'PLN':Z_pln}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a0a7dba1",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fab346a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:11.960724Z",
     "start_time": "2024-06-11T14:27:08.912634Z"
    },
    "scrolled": false
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "benchmark_params = {'class_weight':'balanced'}\n",
    "multiple_performances_summary(\n",
    "    X_dic,\n",
    "    y,\n",
    "    LogisticRegression, benchmark_params,\n",
    "    seed=seed,\n",
    "    save_name=f'{prefix_labels}_ROC_logistic_regression'\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f46c637f",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f163b2b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:27:54.373176Z",
     "start_time": "2024-06-11T14:27:11.961673Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "benchmark_params = {\n",
    "    'n_estimators':100,\n",
    "    'class_weight':'balanced',\n",
    "}\n",
    "multiple_performances_summary(\n",
    "    X_dic,\n",
    "    y,\n",
    "    RandomForestClassifier, benchmark_params,\n",
    "    seed=seed,\n",
    "    save_name=f'{prefix_labels}_ROC_randomforest'\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24f3dc27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:28:38.252984Z",
     "start_time": "2024-06-11T14:27:54.374244Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "benchmark_params = {\n",
    "    'n_estimators':100,\n",
    "    'class_weight':'balanced'\n",
    "}\n",
    "X_dic_layers = {'Raw $\\ell=6$': X}\n",
    "for i in range(len(K)):\n",
    "    X_dic_layers[f'PLN $\\ell={i+selected_layers[0]}$'] = Z_pln_enc[:, i, :]\n",
    "multiple_performances_summary(\n",
    "    X_dic_layers,\n",
    "    y,\n",
    "    RandomForestClassifier, benchmark_params,\n",
    "    seed=seed,\n",
    "    save_name=f'{prefix_labels}_ROC_randomforest_PLN'\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dbd87e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:29:20.832444Z",
     "start_time": "2024-06-11T14:28:38.254254Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "benchmark_params = {\n",
    "    'n_estimators':100,\n",
    "    'class_weight':'balanced'\n",
    "}\n",
    "X_dic_layers = {'Raw $\\ell=6$': X}\n",
    "for i in range(len(K)):\n",
    "    X_dic_layers[f'PLN-Tree (backward) $\\ell={i+selected_layers[0]}$'] = backward.latent_tree_allocation(Z_backward).detach()[:, i, :]\n",
    "multiple_performances_summary(\n",
    "    X_dic_layers,\n",
    "    y,\n",
    "    RandomForestClassifier, benchmark_params,\n",
    "    seed=seed,\n",
    "    save_name=f'{prefix_labels}_ROC_randomforest_PLN-Tree_backward'\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "36f22287",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6d823f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:29:56.934094Z",
     "start_time": "2024-06-11T14:29:20.833525Z"
    }
   },
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "benchmark_params = {\n",
    "    'probability':True,\n",
    "    'kernel':'linear',\n",
    "    'C':0.1,\n",
    "    'class_weight':'balanced'\n",
    "}\n",
    "multiple_performances_summary(\n",
    "    X_dic,\n",
    "    y,\n",
    "    SVC, benchmark_params,\n",
    "    seed=seed,\n",
    "    save_name=f'{prefix_labels}_ROC_SVM'\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6c2b1095",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fbacebb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:35:16.848167Z",
     "start_time": "2024-06-11T14:29:56.934910Z"
    }
   },
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "benchmark_params = {\n",
    "    'hidden_layer_sizes':(256, 256, 124),\n",
    "}\n",
    "multiple_performances_summary(\n",
    "    X_dic,\n",
    "    y,\n",
    "    MLPClassifier, benchmark_params,\n",
    "    seed=seed,\n",
    "    save_name=f'{prefix_labels}_ROC_MLP'\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbf538",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
